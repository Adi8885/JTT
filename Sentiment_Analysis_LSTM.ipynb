{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report , accuracy_score , confusion_matrix , precision_score , f1_score\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import forest_minimize, gbrt_minimize, gp_minimize\n",
    "from skopt.callbacks import DeltaYStopper\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as Dataset\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import pprint\n",
    "\n",
    "def PrintException():\n",
    "    exc_type, exc_obj, tb = sys.exc_info()\n",
    "    f = tb.tb_frame\n",
    "    lineno = tb.tb_lineno\n",
    "    filename = f.f_code.co_filename\n",
    "    linecache.checkcache(filename)\n",
    "    line = linecache.getline(filename, lineno, f.f_globals)\n",
    "    print ('EXCEPTION IN ({}, LINE {} \"{}\"): {}'.format(filename, lineno, line.strip(), exc_obj))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classification_report(y_actual , y_predicted):\n",
    "    print(classification_report(y_actual, y_pred))\n",
    "    acc = accuracy_score(y_actual , y_predicted)\n",
    "    print('accuracy : {}'.format(acc))\n",
    "   \n",
    "    conf_mat=confusion_matrix(y_actual, y_pred)\n",
    "    #print(conf_mat)\n",
    "    #plt.figure(figsize=(20,20))\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(conf_mat, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(le.classes_)\n",
    "    ax.yaxis.set_ticklabels(le.classes_)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    #plt.savefig('conf_matrix.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment         airline  \\\n",
       "0           neutral  Virgin America   \n",
       "1          positive  Virgin America   \n",
       "2           neutral  Virgin America   \n",
       "3          negative  Virgin America   \n",
       "4          negative  Virgin America   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./sentiment_analysis.csv'\n",
    "                   , low_memory = False\n",
    "                   )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Virgin America', 'United', 'Southwest', 'Delta', 'US Airways',\n",
       "       'American'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.airline.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.airline_sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x243ebd07390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.groupby('airline_sentiment')['text'].count().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['airline_sentiment','text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT CLEANING\n",
    "\n",
    "#Remove words starting with @ as they aree just references\n",
    "data['cleaned_text'] = data['text'].apply(lambda x: ' '.join(word for word in x.split(' ') if not word.startswith('@')))\n",
    "\n",
    "#Remove .\n",
    "data['cleaned_text'] = data['cleaned_text'].str.lower().replace('.','')\n",
    "\n",
    "#Remove Special Characters\n",
    "data['cleaned_text'] = data['cleaned_text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14640.000000\n",
       "mean        16.685587\n",
       "std          6.948965\n",
       "min          1.000000\n",
       "25%         11.000000\n",
       "50%         18.000000\n",
       "75%         22.000000\n",
       "max         35.000000\n",
       "Name: seq_length, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['seq_length'] = data['cleaned_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data.seq_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>seq_length</th>\n",
       "      <th>airline_sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>what said</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus youve added commercials to the experience...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>i didnt today must mean i need to take another...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>its really aggressive to blast obnoxious enter...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  \\\n",
       "0           neutral                @VirginAmerica What @dhepburn said.   \n",
       "1          positive  @VirginAmerica plus you've added commercials t...   \n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3          negative  @VirginAmerica it's really aggressive to blast...   \n",
       "4          negative  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "                                        cleaned_text  seq_length  \\\n",
       "0                                          what said           2   \n",
       "1  plus youve added commercials to the experience...           8   \n",
       "2  i didnt today must mean i need to take another...          11   \n",
       "3  its really aggressive to blast obnoxious enter...          16   \n",
       "4            and its a really big bad thing about it           9   \n",
       "\n",
       "   airline_sentiment_encoded  \n",
       "0                          1  \n",
       "1                          2  \n",
       "2                          1  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "data['airline_sentiment_encoded'] = le.fit_transform( data['airline_sentiment'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to process : 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "st_time = time.time()\n",
    "sentence_corpus = []\n",
    "for i in data.cleaned_text:\n",
    "    sentence_corpus.append(i.split())\n",
    "len(sentence_corpus)\n",
    "print('time taken to process : {} seconds'.format(round(time.time() - st_time ,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size : 15884\n",
      "time taken to process : 4.12 seconds\n"
     ]
    }
   ],
   "source": [
    "st_time = time.time()\n",
    "vocab = []\n",
    "for i in data.cleaned_text:\n",
    "    for j in i.split():\n",
    "        if j not in vocab:\n",
    "            vocab.append(j)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab_size : {}'.format(vocab_size))\n",
    "print('time taken to process : {} seconds'.format(round(time.time() - st_time ,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15884\n",
      "15884\n",
      "time taken to map vocab : 4.14 seconds\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "for i in enumerate(vocab):\n",
    "    #print(i)\n",
    "    word_to_idx[i[1]] = i[0]\n",
    "    idx_to_word[i[0]] = i[1]\n",
    "print(len(word_to_idx))\n",
    "print(len(idx_to_word))\n",
    "print('time taken to map vocab : {} seconds'.format(round(time.time() - st_time ,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sentence_length : 35\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 35\n",
    "seq_len = max_sentence_length\n",
    "print('max_sentence_length : {}'.format(max_sentence_length))\n",
    "for i in range(0,len(sentence_corpus)):\n",
    "    sentence = sentence_corpus[i]\n",
    "    if len(sentence) < max_sentence_length :\n",
    "        padding_length = max_sentence_length - len(sentence)\n",
    "        #print(padding_length)\n",
    "        for j in range(len(sentence) , max_sentence_length) :\n",
    "            sentence.append('<>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to train word2vec : 1.37 seconds\n"
     ]
    }
   ],
   "source": [
    "vec_size = 256\n",
    "st_time = time.time()\n",
    "word2vec = Word2Vec(sentence_corpus, window=3 , min_count=1, size=vec_size , workers=os.cpu_count())\n",
    "print('time taken to train word2vec : {} seconds'.format(round(time.time() - st_time ,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    sentence = sentence.split()\n",
    "    input_data = np.zeros((seq_len,vec_size))\n",
    "    #print(corpus)\n",
    "    k = 0\n",
    "    for i in sentence:\n",
    "        if k < seq_len : \n",
    "            if i == '<>':\n",
    "                word_vector = np.zeros(vec_size)\n",
    "            else:\n",
    "                word_vector = word2vec.wv[i] \n",
    "            input_data[k] = word_vector\n",
    "        else:\n",
    "            continue\n",
    "        k+=1\n",
    "        #print(i)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records done : 1000/14640 \t time : 0.32\n",
      "records done : 2000/14640 \t time : 0.32\n",
      "records done : 3000/14640 \t time : 0.31\n",
      "records done : 4000/14640 \t time : 0.33\n",
      "records done : 5000/14640 \t time : 0.31\n",
      "records done : 6000/14640 \t time : 0.31\n",
      "records done : 7000/14640 \t time : 0.3\n",
      "records done : 8000/14640 \t time : 0.3\n",
      "records done : 9000/14640 \t time : 0.31\n",
      "records done : 10000/14640 \t time : 0.32\n",
      "records done : 11000/14640 \t time : 0.31\n",
      "records done : 12000/14640 \t time : 0.31\n",
      "records done : 13000/14640 \t time : 0.32\n",
      "records done : 14000/14640 \t time : 0.31\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "print_every = 1000\n",
    "ctr = 0\n",
    "st_time = time.time()\n",
    "total_records = data.shape[0]\n",
    "for idx in data.index :\n",
    "    text = data.loc[idx].cleaned_text\n",
    "    feature = vectorize_sentence(text)\n",
    "    X.append(feature)\n",
    "    ctr+=1\n",
    "    if ctr % print_every == 0 :\n",
    "        print('records done : {}/{} \\t time : {}'.format(ctr , total_records , round(time.time() - st_time ,2)))\n",
    "        st_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['airline_sentiment_encoded'].values\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "x_train = torch.tensor(x_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train ,y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "x_val = torch.tensor(x_val)\n",
    "y_val = torch.tensor(y_val)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val ,y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(le.classes_)\n",
    "batch_size = batch_size\n",
    "drop_out_probability = 0.5\n",
    "input_size = vec_size\n",
    "hidden_size = input_size\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, training on GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('CUDA available, training on GPU')\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('cuda NOT available, training on CPU') \n",
    "    \n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN_Classifier(\n",
       "  (lstm1): LSTM(256, 256, batch_first=True, dropout=0.5)\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (dropout1): Dropout(p=0.5)\n",
       "  (out): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NN_Classifier(nn.Module):\n",
    "    def __init__(self , input_size , hidden_size , num_layers , output_size):\n",
    "        super(NN_Classifier ,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        #lstm layer\n",
    "        self.lstm1 = nn.LSTM(input_size = input_size\n",
    "                             , hidden_size = hidden_size \n",
    "                             , num_layers = num_layers\n",
    "                             , batch_first = True \n",
    "                             ,dropout=drop_out_probability\n",
    "                             )\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.fc1 = nn.Linear(in_features = input_size , out_features = hidden_size)\n",
    "        \n",
    "        #Droput layer before output\n",
    "        self.dropout1 = nn.Dropout(p = drop_out_probability)\n",
    "        \n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.out = nn.Linear(in_features = hidden_size , out_features = output_size)\n",
    "        \n",
    "    def _init_hidden(self,batch_size):\n",
    "        hidden = torch.zeros((self.num_layers, batch_size , self.hidden_size),dtype = torch.float64)\n",
    "        c_0 = torch.zeros((self.num_layers, batch_size , self.hidden_size),dtype = torch.float64)\n",
    "        return hidden.to(device) , c_0.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #create initial hidden layer of zeros and intial state of zeros\n",
    "        hidden , c_0 = self._init_hidden(batch_size)\n",
    "        \n",
    "        # Pass the input tensor through LSTM\n",
    "        lstm_out ,hidden = self.lstm1(x,(hidden,c_0))\n",
    "        \n",
    "        #Transpose tensor before feeding into Linear layer\n",
    "        lstm_out = lstm_out.transpose(dim0 = 0, dim1 = 1)\n",
    "        x = self.fc1(lstm_out[-1])\n",
    "        \n",
    "        #Dropout layer\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        #Fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #Output layer\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = NN_Classifier(input_size = input_size \n",
    "                        , hidden_size = hidden_size\n",
    "                        , output_size = output_size\n",
    "                        , num_layers = num_layers)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\tbatch: 0/1647\ttrain loss : 1.1132228821309043\ttime : 1.92 secs \n",
      "epoch: 1/10\tbatch: 200/1647\ttrain loss : 0.9311896142227445\ttime : 11.78 secs \n",
      "epoch: 1/10\tbatch: 400/1647\ttrain loss : 0.5758089386704686\ttime : 11.82 secs \n",
      "epoch: 1/10\tbatch: 600/1647\ttrain loss : 0.7281715364847461\ttime : 11.59 secs \n",
      "epoch: 1/10\tbatch: 800/1647\ttrain loss : 1.0459899898790528\ttime : 11.54 secs \n",
      "epoch: 1/10\tbatch: 1000/1647\ttrain loss : 0.9411381210365988\ttime : 11.56 secs \n",
      "epoch: 1/10\tbatch: 1200/1647\ttrain loss : 0.874302031457715\ttime : 11.56 secs \n",
      "epoch: 1/10\tbatch: 1400/1647\ttrain loss : 0.6495979032837436\ttime : 11.6 secs \n",
      "epoch: 1/10\tbatch: 1600/1647\ttrain loss : 0.9947122638537768\ttime : 11.35 secs \n",
      "\n",
      "epoch : 1/10\t train_loss :0.8230985715500054\t validation loss : 0.7732840727298247\n",
      "epoch : 1/10 \t train loss : 0.8230985715500054 \t time required : 7.07 secs\n",
      "\n",
      "epoch: 2/10\tbatch: 0/1647\ttrain loss : 0.747663651289976\ttime : 0.74 secs \n",
      "epoch: 2/10\tbatch: 200/1647\ttrain loss : 0.8724231945538949\ttime : 11.32 secs \n",
      "epoch: 2/10\tbatch: 400/1647\ttrain loss : 0.5029239211729744\ttime : 11.46 secs \n",
      "epoch: 2/10\tbatch: 600/1647\ttrain loss : 0.6368337740662193\ttime : 11.57 secs \n",
      "epoch: 2/10\tbatch: 800/1647\ttrain loss : 0.98747607521535\ttime : 11.46 secs \n",
      "epoch: 2/10\tbatch: 1000/1647\ttrain loss : 0.9199069241152339\ttime : 11.49 secs \n",
      "epoch: 2/10\tbatch: 1200/1647\ttrain loss : 0.8722025670932315\ttime : 11.87 secs \n",
      "epoch: 2/10\tbatch: 1400/1647\ttrain loss : 0.7110829495086903\ttime : 11.56 secs \n",
      "epoch: 2/10\tbatch: 1600/1647\ttrain loss : 0.8112233887982881\ttime : 11.47 secs \n",
      "patience_ctr : 1\n",
      "\n",
      "epoch : 2/10\t train_loss :0.7358061259902127\t validation loss : 0.7501660277731378\n",
      "epoch : 2/10 \t train loss : 0.7358061259902127 \t time required : 7.07 secs\n",
      "\n",
      "epoch: 3/10\tbatch: 0/1647\ttrain loss : 0.7367808172586705\ttime : 0.75 secs \n",
      "epoch: 3/10\tbatch: 200/1647\ttrain loss : 0.782678722421054\ttime : 11.4 secs \n",
      "epoch: 3/10\tbatch: 400/1647\ttrain loss : 0.43264802573609723\ttime : 11.44 secs \n",
      "epoch: 3/10\tbatch: 600/1647\ttrain loss : 0.636832467249733\ttime : 11.39 secs \n",
      "epoch: 3/10\tbatch: 800/1647\ttrain loss : 0.9987397140164037\ttime : 11.33 secs \n",
      "epoch: 3/10\tbatch: 1000/1647\ttrain loss : 0.9814572768648737\ttime : 11.61 secs \n",
      "epoch: 3/10\tbatch: 1200/1647\ttrain loss : 0.9065954962508217\ttime : 11.51 secs \n",
      "epoch: 3/10\tbatch: 1400/1647\ttrain loss : 0.6983451088781951\ttime : 11.45 secs \n",
      "epoch: 3/10\tbatch: 1600/1647\ttrain loss : 0.9782166003567934\ttime : 11.33 secs \n",
      "patience_ctr : 2\n",
      "\n",
      "epoch : 3/10\t train_loss :0.7108398584620608\t validation loss : 0.7566818680939873\n",
      "epoch : 3/10 \t train loss : 0.7108398584620608 \t time required : 7.03 secs\n",
      "\n",
      "epoch: 4/10\tbatch: 0/1647\ttrain loss : 0.6898878334797046\ttime : 0.79 secs \n",
      "epoch: 4/10\tbatch: 200/1647\ttrain loss : 0.7049241328856143\ttime : 11.56 secs \n",
      "epoch: 4/10\tbatch: 400/1647\ttrain loss : 0.37918728476068037\ttime : 11.71 secs \n",
      "epoch: 4/10\tbatch: 600/1647\ttrain loss : 0.6649551039256467\ttime : 11.45 secs \n",
      "epoch: 4/10\tbatch: 800/1647\ttrain loss : 0.9623698602027624\ttime : 11.41 secs \n",
      "epoch: 4/10\tbatch: 1000/1647\ttrain loss : 0.7834843496884655\ttime : 11.43 secs \n",
      "epoch: 4/10\tbatch: 1200/1647\ttrain loss : 0.8582110462750142\ttime : 11.54 secs \n",
      "epoch: 4/10\tbatch: 1400/1647\ttrain loss : 0.675949198462187\ttime : 11.43 secs \n",
      "epoch: 4/10\tbatch: 1600/1647\ttrain loss : 0.9401678331049237\ttime : 11.49 secs \n",
      "patience_ctr : 3\n",
      "\n",
      "epoch : 4/10\t train_loss :0.6856882641758322\t validation loss : 0.6960507684435726\n",
      "epoch : 4/10 \t train loss : 0.6856882641758322 \t time required : 7.11 secs\n",
      "\n",
      "epoch: 5/10\tbatch: 0/1647\ttrain loss : 0.5083631076066188\ttime : 0.77 secs \n",
      "epoch: 5/10\tbatch: 200/1647\ttrain loss : 0.6204203971653425\ttime : 11.46 secs \n",
      "epoch: 5/10\tbatch: 400/1647\ttrain loss : 0.4086231953889941\ttime : 12.09 secs \n",
      "epoch: 5/10\tbatch: 600/1647\ttrain loss : 0.7962921903923768\ttime : 12.14 secs \n",
      "epoch: 5/10\tbatch: 800/1647\ttrain loss : 0.9473244867306986\ttime : 12.11 secs \n",
      "epoch: 5/10\tbatch: 1000/1647\ttrain loss : 0.848855234985861\ttime : 12.11 secs \n",
      "epoch: 5/10\tbatch: 1200/1647\ttrain loss : 0.9748490939672049\ttime : 12.22 secs \n",
      "epoch: 5/10\tbatch: 1400/1647\ttrain loss : 0.6930517052501665\ttime : 12.18 secs \n",
      "epoch: 5/10\tbatch: 1600/1647\ttrain loss : 0.8624746756833133\ttime : 11.52 secs \n",
      "patience_ctr : 4\n",
      "\n",
      "epoch : 5/10\t train_loss :0.6570139306439818\t validation loss : 0.6719081953452136\n",
      "epoch : 5/10 \t train loss : 0.6570139306439818 \t time required : 7.12 secs\n",
      "\n",
      "epoch: 6/10\tbatch: 0/1647\ttrain loss : 0.4118612738081501\ttime : 0.78 secs \n",
      "epoch: 6/10\tbatch: 200/1647\ttrain loss : 0.7273932544562309\ttime : 11.57 secs \n",
      "epoch: 6/10\tbatch: 400/1647\ttrain loss : 0.4142407664669734\ttime : 11.54 secs \n",
      "epoch: 6/10\tbatch: 600/1647\ttrain loss : 0.6745255269230541\ttime : 11.75 secs \n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "print_every = 200\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "# to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = [] \n",
    "patience_ctr = 0\n",
    "patience = 5\n",
    "\n",
    "\n",
    "st_time = time.time()\n",
    "    \n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "\n",
    "    batch = 0\n",
    "    #loss = 0\n",
    "    st_time = time.time()\n",
    "    model.train()\n",
    "    for x, y in train_dataloader :\n",
    "        x = x.to(device)\n",
    "        y = y.long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        #y_pred = y_pred.unsqueeze(0)\n",
    "        loss = loss_fn(y_pred , y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if (batch % print_every == 0):\n",
    "            print('epoch: {}/{}\\tbatch: {}/{}\\ttrain loss : {}\\ttime : {} secs '.format(epoch, \n",
    "                                                                                       epochs , \n",
    "                                                                                       batch,\n",
    "                                                                                       len(train_dataloader) , \n",
    "                                                                                       loss.item() , \n",
    "                                                                                       round(time.time() - st_time , 2)))\n",
    "            st_time = time.time()\n",
    "            \n",
    "        batch +=1\n",
    "\n",
    "    #Calculate Validation loss\n",
    "    val_acc = 0\n",
    "    model = model.eval()\n",
    "    for x_v, y_v in val_dataloader :\n",
    "        x_v = x_v.to(device)\n",
    "        y_v = y_v.long().to(device)\n",
    "        \n",
    "        y_pred = model(x_v)\n",
    "        loss = loss_fn(y_pred , y_v)\n",
    "        \n",
    "        valid_losses.append(loss.item())\n",
    "    \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "    #Early stopping\n",
    "    if valid_loss > train_loss:\n",
    "        patience_ctr +=1\n",
    "        print('patience_ctr : {}'.format(patience_ctr))\n",
    "        if patience_ctr >= patience:\n",
    "            print('early stoppping since valid_loss > train_loss')\n",
    "            break\n",
    "    else :\n",
    "        patience_ctr = 0\n",
    "    \n",
    "    epoch_len = len(str(epochs))\n",
    "    print('\\nepoch : {}/{}\\t train_loss :{}\\t validation loss : {}'.format(epoch , \n",
    "                                                                           epochs, \n",
    "                                                                           train_loss , \n",
    "                                                                           valid_loss))\n",
    "    \n",
    "            \n",
    "    print('epoch : {}/{} \\t train loss : {} \\t time required : {} secs\\n'.format(epoch, \n",
    "                                                                          epochs, \n",
    "                                                                          train_loss,\n",
    "                                                                          round(time.time() - st_time , 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_time = time.time()\n",
    "ctr = 0\n",
    "print_every = 50\n",
    "model = model.eval()\n",
    "for x_v, y_v in val_dataloader :\n",
    "    x_v = x_v.to(device)\n",
    "    #y_v = y_v.to(device)\n",
    "   \n",
    "    op = model(x_v)\n",
    "    op = torch.argmax(F.softmax(op , dim = 1), dim = 1)\n",
    "    if ctr == 0:\n",
    "        y_pred = op\n",
    "        y_actual = y_v\n",
    "    else:\n",
    "        y_pred = torch.cat((y_pred , op))\n",
    "        y_actual = torch.cat((y_actual , y_v))\n",
    "   \n",
    "    ctr += 1\n",
    "   \n",
    "    if ctr % print_every == 0:\n",
    "        print(ctr)\n",
    "print('time taken for prediction :{} seconds'.format(time.time() - st_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = y_val\n",
    "y_pred = y_pred.cpu().detach().numpy()\n",
    "my_classification_report(y_actual = y_actual , y_predicted = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
